# -*- coding: utf-8 -*-
"""A-Z medicine dataset

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IT0RMTrJiJnNR-QtbjKfEhpZrnYYUdQP
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('/content/A_Z_medicines_dataset_of_India.csv')
df

df.head()

df.info()

df.nunique()

df.isnull().sum()

df.describe()

df.shape

df.columns

"""**Handling missing values with 0 cause the that column also needed for application**

"""

df.fillna(0, inplace=True)

df.isnull().sum()

df.duplicated().sum()

plt.figure(figsize=(12, 6))
sns.boxplot(data=df)
plt.xticks(rotation=90)
plt.title('Box Plot for Outlier Detection')
plt.show()

"""**EDA**"""

# Created a histogram to visualize the distribution of medicine prices
plt.figure(figsize=(10, 6))
plt.hist(df['price(₹)'], bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Medicine Prices')
plt.xlabel('Price (₹)')
plt.ylabel('Number of Medicines')
plt.grid()
plt.show()

"""**Feature engineering**"""

#feature drop code

df = df.drop(['price(₹)'], axis=1)

df.head()

plt.figure(figsize=(10, 6))
# Convert the boolean values to integers before plotting
plt.hist(df['Is_discontinued'].astype(int), bins=20, color='skyblue', edgecolor='black')
plt.title('Distribution of Numerical Ratings')
plt.xlabel('Is_discontinued')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

sns.pairplot(df)
plt.show()

plt.figure(figsize=(8, 6))
df['Is_discontinued'].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])
plt.title('Distribution of Discontinued Medicines')
plt.xlabel('Is Discontinued')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.grid(axis='y')
plt.show()

"""**Encoding**"""

from sklearn.preprocessing import LabelEncoder

# Initialize LabelEncoder
le = LabelEncoder()

# Apply label encoding to the specified columns
for col in ['name', 'Is_discontinued', 'manufacturer_name', 'type', 'pack_size_label', 'short_composition1', 'short_composition2']:
    # Convert all values to strings before fitting
    df[col] = df[col].astype(str)
    df[col] = le.fit_transform(df[col])

# Display the updated DataFrame
df.head()

"""**Scaling**"""

from sklearn.preprocessing import StandardScaler

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the numerical features
df[['name','manufacturer_name', 'type', 'pack_size_label', 'short_composition1', 'short_composition2']] = scaler.fit_transform(df[['name','manufacturer_name', 'type', 'pack_size_label', 'short_composition1', 'short_composition2']])

# Display the scaled DataFrame
df.head()

plt.figure(figsize=(15, 10))
sns.heatmap(df.corr(), annot=True, cmap='viridis')
plt.title('Correlation Heatmap')
plt.show()

#sns.pairplot(df, hue='Is_discontinued')
#plt.show()

from sklearn.model_selection import train_test_split

X = df.drop('Is_discontinued', axis=1)
y = df['Is_discontinued']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape,y_train.shape,X_test.shape,y_test.shape

from sklearn.utils.multiclass import type_of_target
type_of_target(y)

"""**1. Decision Tree Classifier**"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, f1_score, precision_score

dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train, y_train)

y_pred = dt_classifier.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)

print(f"Accuracy: {accuracy:}")
print(f"F1 Score: {f1:}")
print(f"Precision: {precision:}")

"""**2. KNeighbours Classifier**"""

from sklearn.neighbors import KNeighborsClassifier

knn_classifier = KNeighborsClassifier(n_neighbors=5)
knn_classifier.fit(X_train, y_train)

y_pred_knn = knn_classifier.predict(X_test)

accuracy_knn = accuracy_score(y_test, y_pred_knn)
f1_knn = f1_score(y_test, y_pred_knn)
precision_knn = precision_score(y_test, y_pred_knn)

print(f"KNN Accuracy: {accuracy_knn}")
print(f"KNN F1 Score: {f1_knn}")
print(f"KNN Precision: {precision_knn}")

"""**3. RandomForest Classifier**"""

from sklearn.ensemble import RandomForestClassifier

rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train, y_train)

y_pred_rf = rf_classifier.predict(X_test)

accuracy_rf = accuracy_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf)

print(f"Random Forest Accuracy: {accuracy_rf}")
print(f"Random Forest F1 Score: {f1_rf}")
print(f"Random Forest Precision: {precision_rf}")

"""**4. Logistic Regression**"""

from sklearn.linear_model import LogisticRegression

lr_classifier = LogisticRegression(random_state=42)
lr_classifier.fit(X_train, y_train)

y_pred_lr = lr_classifier.predict(X_test)

accuracy_lr = accuracy_score(y_test, y_pred_lr)
f1_lr = f1_score(y_test, y_pred_lr)
precision_lr = precision_score(y_test, y_pred_lr)

print(f"Logistic Regression Accuracy: {accuracy_lr}")
print(f"Logistic Regression F1 Score: {f1_lr}")
print(f"Logistic Regression Precision: {precision_lr}")

from sklearn.model_selection import GridSearchCV

# # Define the parameter grid for Random Forest
# param_grid =  {
#     'n_estimators': [100, 200, 300],
#     'max_depth': [5, 10, 15],
#     'min_samples_split': [2, 5, 10],
#     'min_samples_leaf': [1, 2, 4]
# }


# # Create the Random Forest classifier
# rf_model = RandomForestClassifier()

# # Create the GridSearchCV object
# grid_search = GridSearchCV(estimator=rf_model,
#                            param_grid=param_grid,
#                            cv=5,
#                            n_jobs=-1,
#                            scoring='accuracy')

# # Fit the grid search to the data
# grid_search.fit(X_train, y_train)

# # Get the best parameters and best score
# best_params = grid_search.best_params_
# print("Best parameters found: ",best_params)
# best_score = grid_search.best_score_
# print("Best score found: ",best_score)

# # Evaluate the best model on the test set
# best_model = grid_search.best_estimator_
# y_pred_best = best_model.predict(X_test)
# accuracy_best = accuracy_score(y_test, y_pred_best)
# f1_best = f1_score(y_test, y_pred_best)
# precision_best = precision_score(y_test, y_pred_best)

# print(f"Best Model Accuracy: {accuracy_best}")
# print(f"Best Model F1 Score: {f1_best}")
# print(f"Best Model Precision: {precision_best}")

from sklearn.metrics import confusion_matrix


# # Calculate the confusion matrix
# cm = confusion_matrix(y_test, y_pred_rf)  # Replace y_pred_rf with the predictions of your chosen model

# # Plot the confusion matrix
# plt.figure(figsize=(8, 6))
# sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
#             xticklabels=['Not Discontinued', 'Discontinued'],
#             yticklabels=['Not Discontinued', 'Discontinued'])
# plt.title("Confusion Matrix")
# plt.xlabel("Predicted Label")
# plt.ylabel("True Label")
# plt.show()

import pickle

# Save the trained model to a file
with open('random_model.pkl', 'wb') as file:
    pickle.dump(rf_classifier, file)